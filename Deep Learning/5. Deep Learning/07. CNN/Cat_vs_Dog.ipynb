{"cells":[{"cell_type":"markdown","source":"\n<div style=\"background:#222222; color:#ffffff; padding:20px\">\n    <h2 align=\"center\"> Cats vs Dogs </h2>\n    <h2 align=\"center\" style=\"color:#01ff84\"> Loading your dataset</h2>\n<div>\n\n<br>\n    \nSo far we've been working with fairly artificial datasets that you wouldn't typically be using in real projects. Instead, you'll likely be dealing with full-sized images like you'd get from smart phone cameras. In this notebook, we'll look at how to load images and use them to train neural networks.\n\nWe'll be using a [dataset of cat and dog photos](https://www.kaggle.com/c/dogs-vs-cats) available from Kaggle. Here are a couple example images:\n\n<img src=\"imgs/dog.png\" width=\"200\" height=\"40\" />\n<img src=\"imgs/cat.png\" width=\"200\" height=\"40\" />\n    \n\nWe'll use this dataset to train a neural network that can differentiate between cats and dogs. These days it doesn't seem like a big accomplishment, but five years ago it was a serious challenge for computer vision systems.\n    \n<br>\n    \nStart importing the needed libraries:\n    \n","metadata":{"cell_id":"00000-c87dd5a5-03ed-47e2-baf2-e6e9d3ba135c","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00001-c2457c17-3372-4f5f-894a-0e4d5ad80223","deepnote_to_be_reexecuted":false,"source_hash":"5d121ebb","execution_start":1614685639776,"execution_millis":956,"deepnote_cell_type":"code"},"source":"%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\nimport matplotlib.pyplot as plt\n\nimport torch\nfrom torchvision import datasets, transforms\n","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The easiest way to load image data is with `datasets.ImageFolder` from `torchvision` ([documentation](http://pytorch.org/docs/master/torchvision/datasets.html#imagefolder)). In general you'll use `ImageFolder` like so:\n\n```python\ndataset = datasets.ImageFolder('path/to/data', transform=transform)\n```\n\nwhere `'path/to/data'` is the file path to the data directory and `transform` is a list of processing steps built with the [`transforms`](http://pytorch.org/docs/master/torchvision/transforms.html) module from `torchvision`. ImageFolder expects the files and directories to be constructed like so:\n```\nroot/dog/xxx.png\nroot/dog/xxy.png\nroot/dog/xxz.png\n\nroot/cat/123.png\nroot/cat/nsdf3.png\nroot/cat/asd932_.png\n```\n\nwhere each class has it's own directory (`cat` and `dog`) for the images. The images are then labeled with the class taken from the directory name. So here, the image `123.png` would be loaded with the class label `cat`. You can download the dataset already structured like this [from here](https://s3.amazonaws.com/content.udacity-data.com/nd089/Cat_Dog_data.zip). They are already splitted into a training set and test set.\n\n \n>**Exercise:** Download the dataset and place the train and test set in the `datasets/cat_vs_dog` folder. If you're cloning this from github, you should have it in `../datasets/`. So first create the `cat_vs_dog` folder in `datasets` and verify that the data are there by running `ls ../datasets/cat_vs_dog` (or your custom path if you changed it).","metadata":{"cell_id":"00002-4a6dff14-0914-4bd9-9976-19f25095e695","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00003-ccb3143b-0ff0-42bc-8c15-33a454575159","deepnote_cell_type":"code"},"source":"","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Great! Now that you have downloaded your data, you need to define the transformations to be passed to the `ImageFolder` function. You have already used them with the MNIST dataset (see the Data Augmentation workbook in Pytorch). While for MNIST you were passing the transformation in the following line of code\n\n`datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)`\n\nhere you do it in the `ImageFolder` method. You can think it as a way to work on every dataset, not only on the MNIST one.\n\n\n### Transforms\n\nWhen you load in the data with `ImageFolder`, you'll need to define some transforms. For example, the images are different sizes but we'll need them to all be the same size for training. You can either resize them with `transforms.Resize()` or crop with `transforms.CenterCrop()`, `transforms.RandomResizedCrop()`, etc. We'll also need to convert the images to PyTorch tensors with `transforms.ToTensor()`. Typically you'll combine these transforms into a pipeline with `transforms.Compose()`, which accepts a list of transforms and runs them in sequence. \n\nAs in the other notebook, you can use the following transformations:\n\n```python\ntransform = transforms.Compose([transforms.RandomRotation(30),\n                                       transforms.RandomResizedCrop(224),\n                                       transforms.RandomHorizontalFlip(),\n                                       transforms.ToTensor(),\n                                       transforms.Normalize([0.5, 0.5, 0.5], \n                                                            [0.5, 0.5, 0.5])])\n\n```\n\n**WARNING!** Remember that transformation are super useful for \"augmenting\" your training data, so that you make your network less vulnerable to different sizes, rotations, or cropping. However, when you are on the test data, there is no need of augmenting the data! Actually, it is not a good practice to do that because there would be very repetitive test data that invalidates your score.\n\nFor this reason, define two different transformations for training and test data (remember that `ToTensor()` and normalization are necessary also for the test data, as well as the resizing (you can use `transforms.Resize(size)` for it):\n\n","metadata":{"cell_id":"00004-ee2c848a-ecc5-4d7d-a9b7-9747e4ff404f","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00005-20de4af3-a74c-4b7a-a32c-ce405a785084","deepnote_cell_type":"code"},"source":"train_transform = # your code here\n\ntest_transform = # your code here\n\n\ndata_dir = \"../datasets/cat_vs_dog\" # or the path where you have downloaded the dataset\n\ntrain_data = datasets.ImageFolder(data_dir + '/train', transform=train_transforms)\ntest_data = datasets.ImageFolder(data_dir + '/test', transform=test_transforms)","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now that you have you have defined the needed transformation, it's time to build the Data loader itself!\n\n### Data Loaders\n\nWith the `ImageFolder` loaded, you have to pass it to a [`DataLoader`](http://pytorch.org/docs/master/data.html#torch.utils.data.DataLoader). The `DataLoader` takes a dataset (such as you would get from `ImageFolder`) and returns batches of images and the corresponding labels. You can set various parameters like the batch size and if the data is shuffled after each epoch.\n\n```python\ndataloader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)\n```\n\nHere `dataloader` is a [generator](https://jeffknupp.com/blog/2013/04/07/improve-your-python-yield-and-generators-explained/). To get data out of it, you need to loop through it or convert it to an iterator and call `next()`.\n\n```python\n# Looping through it, get a batch on each loop \nfor images, labels in dataloader:\n    pass\n\n# Get one batch\nimages, labels = next(iter(dataloader))\n```\n \n>**Exercise:** Build the dataloader for both the train and test data. Choose the batch size that fits your memory. \n**Remember** NOT TO shuffle the test data! ","metadata":{"cell_id":"00006-403d74bd-0cd7-47bf-9043-6c3229c8b737","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00007-3489d718-69e4-4adc-8b62-3a5c3efa98be","deepnote_cell_type":"code"},"source":"trainloader = # your code here\ntestloader = # your code here","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"00008-ba022cd0-7bf6-4cc6-affc-f1cd5242427c","deepnote_cell_type":"code"},"source":"def imshow(image, ax=None, title=None, normalize=True):\n    \"\"\"Imshow for Tensor.\"\"\"\n    if ax is None:\n        fig, ax = plt.subplots()\n    image = image.numpy().transpose((1, 2, 0))\n\n    if normalize:\n        mean = np.array([0.485, 0.456, 0.406])\n        std = np.array([0.229, 0.224, 0.225])\n        image = std * image + mean\n        image = np.clip(image, 0, 1)\n\n    ax.imshow(image)\n    ax.spines['top'].set_visible(False)\n    ax.spines['right'].set_visible(False)\n    ax.spines['left'].set_visible(False)\n    ax.spines['bottom'].set_visible(False)\n    ax.tick_params(axis='both', length=0)\n    ax.set_xticklabels('')\n    ax.set_yticklabels('')\n\n    return ax\n\n\n# Run this to test your data loaders\nimages, labels = next(iter(trainloader))\nimshow(images[0], normalize=False)\n\n","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Ok, now let's create a simple Convolutional Neural Network for this task!","metadata":{"cell_id":"00009-1d5d3d61-c4c9-4a9b-92e4-dad35beb3172","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00010-7f752750-6dcd-4877-b2c8-38c63d5851d8","deepnote_cell_type":"code"},"source":"import torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(3, 32, 5)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, 2)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = x.view(-1, 16 * 5 * 5)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        x = LogSoftmax(x, dim=1)\n        return x\n\ncriterion = nn.NLLLoss()\noptimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n\nnet = Net()","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Why do we reshape in the middle of the forward pass?\n\nAs you remember from the class, the output of a convolutional layer is always a 3D volume! For this reason, since the output channels of the conv2 layer is 16 and the feature maps have size 5x5, then the input of the fc1 layer must be reshaped to have shape `(batch_size, 16 * 5 * 5)`.\n\nNow the question is: what will be the difference in the training of this network with respect to the fully-connected one you are used to?\n\nNone, except for the fact that you do not reshape the input to be a vector, but you keep the shape as a volume! \n\n>**Exercise:** Implement a Convolutional Neural Network for the cat vs dog challenge, such that:\n> - The input images have shape 28x28 and three RGB channels\n> - You have 2 Conv2d layer with MaxPool2D in the middle and two fully-connected layer at the end.\n> - You can decide yourself the rest of the hyperparameters (kernel size, number of filters...)\n> - Train and evaluate your model\n\nFollowing there's an helper function to visualize your prediction once your model has been built.\n\n","metadata":{"cell_id":"00011-6d0c6b61-70ec-4075-ac82-db1fdca7dbcc","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00012-68330ba2-826c-44d1-b343-6ea944ea0c7b","deepnote_cell_type":"code"},"source":"class_list = train_data.classes\n\ndef view_classify_general(img, ps, class_list):\n    ''' Function for viewing an image and it's predicted classes.\n    '''\n    ps = ps.data.numpy().squeeze()\n\n    fig, (ax1, ax2) = plt.subplots(figsize=(6,9), ncols=2)\n    helper.imshow(img, ax=ax1, normalize=True)\n    ax1.axis('off')\n    ax2.barh(np.arange(len(class_list)), ps)\n    ax2.set_aspect(0.1)\n    ax2.set_yticks(np.arange(len(class_list)))\n    ax2.set_yticklabels([x for x in class_list], size='small');\n    ax2.set_title('Class Probability')\n    ax2.set_xlim(0, 1.1)\n\n    plt.tight_layout()\n\nimages, labels = next(iter(trainloader))\n# Flatten images\nimg = images.view(images.shape[0], -1)[0]\n# Forward pass, get our logits\nlogits = model(img)\n# Calculate the loss with the logits and the labels\nloss = criterion(logits, labels)\nps = torch.exp(logits)\n    \nview_classify_general(img, ps, class_list)","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"You should get something cute like this:\n\n![image](imgs/cat_pred.png)\n","metadata":{"cell_id":"00013-b644b957-a898-4c27-865d-b2af261c1db4","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"","metadata":{"cell_id":"00014-9182ffba-9cf4-44f1-937f-a736dd6a9c44","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=ccdb9771-ae37-4326-b040-1f894af2ffbd' target=\"_blank\">\n<img style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"tags":[],"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.1"},"deepnote_notebook_id":"1c116dcc-535f-4f0b-90df-14b1c70e6bb8","deepnote":{},"deepnote_execution_queue":[]}}