{"cells":[{"cell_type":"markdown","source":"# Data Augmentation\nA common strategy for training neural networks is to introduce randomness in the input data itself. For example, you can randomly rotate, mirror, scale, and/or crop your images during training. This will help your network generalize as it's seeing the same images but in different locations, with different sizes, in different orientations, etc.\n\nYou can import from torchvision the transforms module:\n\n\n","metadata":{"cell_id":"00000-6fc1f4e1-6d74-4714-995b-c6aa8426f077","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00001-f7501e5b-14fa-432a-96c4-f6aa0e5d0957","deepnote_to_be_reexecuted":false,"source_hash":"1002673d","execution_start":1614684518736,"execution_millis":1231,"deepnote_cell_type":"code"},"source":"from torchvision import transforms","outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Some of the operation you can do with it:\n    \n- Random rotations: `transforms.RandomRotation(degrees)` will perform random rotations of your images \n    The `degrees` parameter is a range of degrees to select from. If degrees is a number instead of sequence like (min, max), the range of degrees will be (-degrees, +degrees).\n\n- Random resize or crop: `torchvision.transforms.RandomResizedCrop(size)`. Crop the given image to random size and aspect ratio. If size is an int, you will get a square image.\n\n- Random Horizontal flip: `transforms.RandomHorizontalFlip()` randomly horizontally flip the images.\n\nThe rest of the available transformations is [here](https://pytorch.org/docs/stable/torchvision/transforms.html) .\n\n\nYou can also compose these transformation using:\n\n\n\n```python\ntrain_transforms = transforms.Compose([transforms.RandomRotation(30),\n                                       transforms.RandomResizedCrop(224),\n                                       transforms.RandomHorizontalFlip(),\n                                       transforms.ToTensor(),\n                                       transforms.Normalize([0.5, 0.5, 0.5], \n                                                            [0.5, 0.5, 0.5])])\n```\n\nYou'll also typically want to normalize images with `transforms.Normalize`. You pass in a list of means and list of standard deviations, then the color channels are normalized like so\n\n```input[channel] = (input[channel] - mean[channel]) / std[channel]```\n\nSubtracting `mean` centers the data around zero and dividing by `std` squishes the values to be between -1 and 1. Normalizing helps keep the network work weights near zero which in turn makes backpropagation more stable. *Without normalization, networks will tend to fail to learn.*\n\n When you're testing however, you'll want to use images that aren't altered (except you'll need to normalize the same way). So, for validation/test images, you'll typically just resize and crop.\n \nLet's try on the MNIST dataset!","metadata":{"cell_id":"00002-6cdb3f3b-2ef6-42a0-a1aa-62a661b5353a","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00003-f6d6afe8-fbe2-47ed-b522-71048c5a2a8d","deepnote_to_be_reexecuted":false,"source_hash":"4847fee1","execution_start":1614684692725,"execution_millis":0,"deepnote_cell_type":"code"},"source":"import torch\nfrom torchvision import datasets, transforms\n\n# Define a transform to normalize the data\ntransform = transforms.Compose([transforms.RandomRotation(30),\n                                       transforms.RandomResizedCrop(16),\n                                       transforms.RandomHorizontalFlip(),\n                                       transforms.ToTensor(),\n                                       transforms.Normalize([0.5], \n                                                            [0.5])])","outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Notice that I'm using `transforms.Normalize([0.5], [0.5])])` and not `transforms.Normalize([0.5,0.5,0.5], [0.5,0.5,0.5])])` because in MNIST there are only grayscale images (so only one color channel).\n\nUsing \n\n```datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)```\n\n\nthe transformation that you have defined must be passed to the `transform` argument as above\n","metadata":{"cell_id":"00004-5da20234-fd3c-4428-a101-73977fd1e8fc","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00005-783f3294-08d7-4fd8-9c7e-73c1a4233d79","deepnote_to_be_reexecuted":false,"source_hash":"6964ef4a","execution_start":1614684732157,"execution_millis":1211,"deepnote_cell_type":"code"},"source":"# Download and load the training data\ntrainset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=16, shuffle=True)","outputs":[{"name":"stdout","text":"Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /root/.pytorch/MNIST_data/MNIST/raw/train-images-idx3-ubyte.gz\n","output_type":"stream"},{"data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"183ccd79f72b41e7bba34c4000f46955"}},"metadata":{},"output_type":"display_data"},{"name":"stdout","text":"Extracting /root/.pytorch/MNIST_data/MNIST/raw/train-images-idx3-ubyte.gz to /root/.pytorch/MNIST_data/MNIST/raw\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to /root/.pytorch/MNIST_data/MNIST/raw/train-labels-idx1-ubyte.gz\n","output_type":"stream"},{"data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"993a266cb89a46a8af6c807b38bc82d4"}},"metadata":{},"output_type":"display_data"},{"name":"stdout","text":"Extracting /root/.pytorch/MNIST_data/MNIST/raw/train-labels-idx1-ubyte.gz to /root/.pytorch/MNIST_data/MNIST/raw\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to /root/.pytorch/MNIST_data/MNIST/raw/t10k-images-idx3-ubyte.gz\n","output_type":"stream"},{"data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5eb5745f46b3409b942c1969da79ff07"}},"metadata":{},"output_type":"display_data"},{"name":"stdout","text":"Extracting /root/.pytorch/MNIST_data/MNIST/raw/t10k-images-idx3-ubyte.gz to /root/.pytorch/MNIST_data/MNIST/raw\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to /root/.pytorch/MNIST_data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n","output_type":"stream"},{"data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb3a17d6094f46e9b910e1b17acc3941"}},"metadata":{},"output_type":"display_data"},{"name":"stdout","text":"Extracting /root/.pytorch/MNIST_data/MNIST/raw/t10k-labels-idx1-ubyte.gz to /root/.pytorch/MNIST_data/MNIST/raw\nProcessing...\nDone!\n/shared-libs/python3.7/py/lib/python3.7/site-packages/torchvision/datasets/mnist.py:480: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","metadata":{"cell_id":"00006-ebbb5d9a-611e-42f4-8ba2-2ceedcd3afe1","deepnote_to_be_reexecuted":false,"source_hash":"ad500d3f","execution_start":1614684747924,"execution_millis":19,"deepnote_cell_type":"code"},"source":"images, labels = next(iter(trainloader))","outputs":[],"execution_count":null},{"cell_type":"code","metadata":{"cell_id":"00007-8a06fbba-4dfc-4dc0-899d-4def81ff5d97","deepnote_to_be_reexecuted":false,"source_hash":"f3b1f977","execution_start":1614684751525,"execution_millis":4,"deepnote_cell_type":"code"},"source":"images.shape","outputs":[{"output_type":"execute_result","execution_count":5,"data":{"text/plain":"torch.Size([16, 1, 16, 16])"},"metadata":{}}],"execution_count":null},{"cell_type":"markdown","source":"As you can see, the shape of the images (last two entries of the cell above) is 16x16 and not 28x28 as expected for the MNIST dataset!","metadata":{"cell_id":"00008-e1a19612-1609-4a56-adab-ba0c93e99446","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00009-60938d29-20b6-480f-bfc1-dc4b4506b90b","deepnote_to_be_reexecuted":false,"source_hash":"30741eaf","execution_start":1614685597245,"execution_millis":400,"deepnote_cell_type":"code"},"source":"import matplotlib.pyplot as plt\n\ndef imshow(image, ax=None, title=None, normalize=True):\n    \"\"\"Imshow for Tensor.\"\"\"\n    if ax is None:\n        fig, ax = plt.subplots()\n    image = image.numpy().transpose((1, 2, 0))\n\n    if normalize:\n        mean = np.array([0.485, 0.456, 0.406])\n        std = np.array([0.229, 0.224, 0.225])\n        image = std * image + mean\n        image = np.clip(image, 0, 1)\n\n    ax.imshow(image)\n    ax.spines['top'].set_visible(False)\n    ax.spines['right'].set_visible(False)\n    ax.spines['left'].set_visible(False)\n    ax.spines['bottom'].set_visible(False)\n    ax.tick_params(axis='both', length=0)\n    ax.set_xticklabels('')\n    ax.set_yticklabels('')\n\n    return ax\n\nimshow(images[0], normalize=False)","outputs":[{"output_type":"execute_result","execution_count":6,"data":{"text/plain":"<AxesSubplot:>"},"metadata":{}},{"data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"},"output_type":"display_data"}],"execution_count":null},{"cell_type":"markdown","source":"Visualizing the image you can see it has been randomly cropped as well!","metadata":{"cell_id":"00010-36db53f0-7360-4158-8319-a2d3683c2ff1","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"You can apply these in your custom dataset as well! Just check the `ImageFolder` method of the `datasets` class in Pytorch:\n    \nhttps://pytorch.org/vision/0.8/datasets.html#imagefolder","metadata":{"cell_id":"00011-7352f371-c5d4-40e4-9019-6c749a7d4a81","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00012-a89881da-f1f7-41a2-b131-9ef1ec7f1aa7","deepnote_cell_type":"code"},"source":"","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=ccdb9771-ae37-4326-b040-1f894af2ffbd' target=\"_blank\">\n<img style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"tags":[],"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.1"},"deepnote_notebook_id":"8e69231c-dbf0-42a0-99f4-5ea244172b33","deepnote":{},"deepnote_execution_queue":[]}}